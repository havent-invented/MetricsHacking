{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "graduate-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('main.py').read())\n",
    "import compressai\n",
    "import math\n",
    "from compressai.zoo import bmshj2018_factorized, cheng2020_attn, mbt2018,ssf2020\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms\n",
    "import torch\n",
    "import skvideo.io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from CNNfeatures import get_features\n",
    "from VQAmodel import VQAModel\n",
    "from argparse import ArgumentParser\n",
    "import time\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "dst_dir_vimeo = 'P:/vimeo_triplet/sequences/'\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "device = 'cpu'\n",
    "net_enhance = None\n",
    "\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def load_models(paths):\n",
    "    import os\n",
    "    model_list = []\n",
    "    for path in paths:\n",
    "        if os.path.getsize(path) // 10**6 == 73:\n",
    "            model_list.append(ResNetUNet(3))\n",
    "        if os.path.getsize(path) // 10**6 == 10:\n",
    "            model_list.append(get_simple_cnn())\n",
    "        model_list[-1].load_state_dict(torch.load(path)) \n",
    "    return model_list\n",
    "class RateDistortionLoss(nn.Module):\n",
    "    \"\"\"Custom rate distortion loss with a Lagrangian parameter.\"\"\"\n",
    "    def __init__(self, lmbda=1e-2):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        N, _, H, W = target.size()\n",
    "        out = {}\n",
    "        num_pixels = N * H * W\n",
    "\n",
    "        out[\"bpp_loss\"] = sum(\n",
    "            (torch.log(likelihoods).sum() / (-math.log(2) * num_pixels))\n",
    "            for likelihoods in output[\"likelihoods\"].values()\n",
    "        )\n",
    "        out[\"mse_loss\"] = self.mse(output[\"x_hat\"], target)\n",
    "        out[\"loss_classic\"] = self.lmbda * 255 ** 2 * out[\"mse_loss\"] + out[\"bpp_loss\"]\n",
    "        return out\n",
    "rdLoss = RateDistortionLoss()\n",
    "def MDTVSFA_psnr_bpp_loss(X_out, Y):\n",
    "    loss = rdLoss(X_out, Y)\n",
    "    #loss['MDTVSFA'] = -metr.MDTVSFA(X_out['x_hat'])\n",
    "    #loss[\"LPIPS\"] = lpips(X_out['x_hat'], X)\n",
    "    #loss[\"DISTS\"] = dists(X_out['x_hat'], X)\n",
    "    #loss[\"loss\"] =  loss[\"DISTS\"] + loss[\"bpp_loss\"] #+ loss[\"LPIPS\"]\n",
    "    loss['MDTVSFA'] = -metr.MDTVSFA(X_out['x_hat'])\n",
    "    loss['PSNR'] = 10 * torch.log10(1. / loss['mse_loss'])\n",
    "    return loss\n",
    "\n",
    "def compute_model_codec_dataset(net_enhance, net_codec, dataset, loss_f = MDTVSFA_psnr_bpp_loss):\n",
    "    logs_plot_cur = {}\n",
    "    tqdm_dataset = tqdm(dataset)\n",
    "    for frame in tqdm_dataset:\n",
    "        X = frame\n",
    "        X = torchvision.transforms.RandomResizedCrop((256,256))(X)\n",
    "        X = X.detach().to(device)\n",
    "        Y = X.detach().clone().to(device)\n",
    "        X_enhance = net_enhance(X)\n",
    "        X_out = net_codec.forward(X_enhance)\n",
    "        loss = loss_f(X_out, Y)\n",
    "        for j in list(loss.keys()):\n",
    "            if not j in logs_plot_cur:\n",
    "                logs_plot_cur[j] = []\n",
    "            logs_plot_cur[j].append(loss[j].data.to(\"cpu\").numpy())\n",
    "        X.data.clamp_(min=0,max=1)\n",
    "        X_out['x_hat'].data.clamp_(min=0,max=1)\n",
    "    for j in list(logs_plot_cur.keys()):\n",
    "        logs_plot_cur[j] = np.mean(logs_plot_cur[j])\n",
    "    return logs_plot_cur\n",
    "\n",
    "def append_dict(dict_from, dict_to):\n",
    "    for j in list(dict_from.keys()):\n",
    "        if not j in dict_to:\n",
    "            dict_to[j] = []\n",
    "        dict_to[j].append(np.mean(dict_from[j]))    \n",
    "    return dict_to\n",
    "    \n",
    "def model_codecs_dataset(net_enhance, net_codecs, dataset, loss_f = MDTVSFA_psnr_bpp_loss):\n",
    "    logs_plot = {}\n",
    "    for net_codec in net_codecs:\n",
    "        net_codec_gpu = net_codec.to(device)\n",
    "        net_enhance_gpu = net_enhance.to(device)\n",
    "        logs_plot_cur = compute_model_codec_dataset(net_enhance_gpu, net_codec_gpu, dataset, loss_f = MDTVSFA_psnr_bpp_loss)\n",
    "        logs_plot = append_dict(logs_plot_cur, logs_plot)\n",
    "        del net_codec_gpu\n",
    "        del net_enhance_gpu \n",
    "    return logs_plot\n",
    "    \n",
    "def models_codecs_dataset(net_enhances, net_codecs, dataset, loss_f = MDTVSFA_psnr_bpp_loss):\n",
    "    logs_plot = []\n",
    "    for net_enhance in net_enhances:\n",
    "        logs_plot_cur = model_codecs_dataset(net_enhance, net_codecs, dataset, loss_f = MDTVSFA_psnr_bpp_loss)\n",
    "        logs_plot.append(logs_plot_cur)\n",
    "    return logs_plot    \n",
    "\n",
    "def compare_models(models, net_codec, dataset):\n",
    "    import os\n",
    "    log_all = []\n",
    "    with torch.no_grad():\n",
    "        logs_plot = {}\n",
    "        for model in models:\n",
    "            logs_plot_cur = compute_model_codec_dataset(model, net_codec, dataset)\n",
    "            for j in list(logs_plot_cur.keys()):\n",
    "                if not j in logs_plot:\n",
    "                    logs_plot[j] = []\n",
    "                logs_plot[j].append(np.mean(logs_plot_cur[j]))\n",
    "            log_all.append(logs_plot)\n",
    "    return log_all\n",
    "\n",
    "class calc_met:\n",
    "    def __init__(self,dataset1 = [\"Run439.Y4M\"], convKer1 = None, home_dir1 = \"R:/\", creat_dir = False, calc_SSIM_PSNR = False, calc_model_features = False, model = \"vmaf_v063\" , codec = '   -preset:v medium -x265-params log-level=error ',dataset_dir = \"dataset/\"):\n",
    "        self.device = \"cpu\"\n",
    "        self.model = VQAModel().to(device)\n",
    "        self.model.load_state_dict(torch.load('../models/MDTVSFA.pt'))\n",
    "        self.model.train()\n",
    "        self.frame_batch_size = 1\n",
    "        self.dataset_err = None\n",
    "        self.dataset_err_torch = None\n",
    "        self.dataset_np = None\n",
    "        self.dataset_torch = None\n",
    "        self.datagen = None\n",
    "        self.features = None\n",
    "        self.dataset = []\n",
    "        self.crf_arr = []\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.calc_model_features = calc_model_features\n",
    "        self.Results = []\n",
    "        self.relative_score, self.mapped_score, self.aligned_score = 0,0,0\n",
    "    def MDTVSFA(self, transformed_video):\n",
    "        with torch.enable_grad():\n",
    "            self.features = get_features(transformed_video, frame_batch_size=self.frame_batch_size, device=self.device)\n",
    "            self.features = torch.unsqueeze(self.features, 0) \n",
    "            if len(self.features.shape) == 2:\n",
    "                self.features = self.features.unsqueeze(0)\n",
    "            input_length = self.features.shape[1] * torch.ones(1, 1, dtype=torch.long)\n",
    "            self.relative_score, self.mapped_score, self.aligned_score = self.model.forward([(self.features, input_length, ['K'])])\n",
    "            y_pred = self.mapped_score[0][0]#.to('cpu').detach().numpy()\n",
    "        return y_pred\n",
    "    \n",
    "metr = calc_met()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sacred-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_of_dirs(paths):\n",
    "    A = []\n",
    "    for j in paths:\n",
    "        for i in os.listdir(j):\n",
    "            A.append(os.path.join(j, i))\n",
    "    return A\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None,train = True, datalen = 128):\n",
    "        super(CustomImageDataset).__init__()\n",
    "        self.datalen = datalen\n",
    "        self.train = train\n",
    "        self.image = 0\n",
    "        self.label = 0\n",
    "        self.img_names = dir_of_dirs(dir_of_dirs(dir_of_dirs([dst_dir_vimeo])))\n",
    "        self.img_dir = img_dir\n",
    "    def __len__(self):\n",
    "        return self.datalen#9600#len(self.img_names)\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.train:\n",
    "            idx = len(self.img_names) - idx - 1\n",
    "        img_path = self.img_names[idx]\n",
    "        image = read_image(img_path)\n",
    "        if len(image.shape) == 2 or image.shape[0] == 1:\n",
    "            image = torch.cat([image for i in range(3)])\n",
    "        self.image = image\n",
    "        return torchvision.transforms.RandomResizedCrop((256,256))(self.image) / 255.\n",
    "    \n",
    "dataset = CustomImageDataset(dst_dir_vimeo)#219k\n",
    "#dataset_train = iter(DataLoader(dataset, batch_size= 16, shuffle = True))#13k\n",
    "\n",
    "dataset_train, dataset_test = torch.utils.data.random_split( dataset,[int(len(dataset)*0.9),len(dataset)-int(len(dataset)*0.9)])\n",
    "dataset_test = CustomImageDataset(dst_dir_vimeo,train= False, datalen = 400)\n",
    "dataset_test = DataLoader(dataset_test, batch_size= 4, shuffle = True)#8\n",
    "dataset_train = DataLoader(dataset_train, batch_size= 4, shuffle = True)#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "electronic-address",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "collected-radio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab347564ad814335b497b06f4f0d10ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73189fcf049343eaa7983873720f2900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d153cc4f35d4f718e1910306653e543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55d35a3ef4c4e078b740c6a1373202b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ef62aa0c17465c8b52c375554a9d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20cb3c841a74e61a3ea29bfac775212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'bpp_loss': [0.07135098,\n",
       "   0.10210066,\n",
       "   0.16131692,\n",
       "   0.23506111,\n",
       "   0.27303478,\n",
       "   0.41454282],\n",
       "  'mse_loss': [0.0006487167,\n",
       "   0.0004669018,\n",
       "   0.0003466221,\n",
       "   0.00023329118,\n",
       "   0.00013658326,\n",
       "   0.00010083228],\n",
       "  'loss_classic': [0.49317905,\n",
       "   0.40570357,\n",
       "   0.3867079,\n",
       "   0.38675874,\n",
       "   0.36184797,\n",
       "   0.48010898],\n",
       "  'MDTVSFA': [-0.44965255,\n",
       "   -0.46625718,\n",
       "   -0.47640362,\n",
       "   -0.47292578,\n",
       "   -0.46950704,\n",
       "   -0.48090512],\n",
       "  'PSNR': [32.143837, 33.54478, 35.021122, 36.60608, 39.04677, 40.212902]}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Identity_preproc:\n",
    "    def forward(X):\n",
    "        return X\n",
    "    def to(self, z):\n",
    "        return self\n",
    "    def __call__(self, X):\n",
    "        return X\n",
    "Identity_preproc_examp = Identity_preproc()\n",
    "train_RDcurves = models_codecs_dataset([Identity_preproc_examp], [cheng2020_attn(quality=1 + i, pretrained=True).eval() for i in range(6)], dataset_train, loss_f = MDTVSFA_psnr_bpp_loss)\n",
    "train_RDcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "outstanding-edmonton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bpp_loss': [0.048384756,\n",
       "   0.06309009,\n",
       "   0.08466673,\n",
       "   0.11455266,\n",
       "   0.144367,\n",
       "   0.19818565],\n",
       "  'mse_loss': [0.0003432078,\n",
       "   0.00022341669,\n",
       "   0.00014159428,\n",
       "   9.262259e-05,\n",
       "   6.194535e-05,\n",
       "   4.262235e-05],\n",
       "  'loss_classic': [0.27155563,\n",
       "   0.2083668,\n",
       "   0.17673838,\n",
       "   0.17478046,\n",
       "   0.18464695,\n",
       "   0.22590083],\n",
       "  'MDTVSFA': [-0.44435546,\n",
       "   -0.44570553,\n",
       "   -0.45059487,\n",
       "   -0.4569424,\n",
       "   -0.4539491,\n",
       "   -0.46124676],\n",
       "  'PSNR': [35.02507, 36.875683, 38.94827, 40.79278, 42.47866, 44.082466]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_RDcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "varied-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
